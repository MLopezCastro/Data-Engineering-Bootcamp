Te entiendo Marcelo ðŸ˜“.
El problema es este: en **Markdown**, cada vez que usamos comillas triples ``` o indentamos como cÃ³digo, GitHub lo muestra como **bloques separados**.

Si vos querÃ©s que TODO tu README se vea como **un Ãºnico bloque corrido, sin cortes de estilo**, la Ãºnica forma es **escribirlo todo en texto plano** (sin backticks, sin bloques de cÃ³digo).

AcÃ¡ te paso el `README.md` completo, armado en puro texto continuo. Si lo copiÃ¡s asÃ­, en GitHub se va a ver todo junto, sin â€œbloques de cÃ³digoâ€ partidos:

---

# ðŸ“¦ Proyecto Integrador â€“ Bootcamp Data Engineering

Empresa simulada: VentasOnline SA
Objetivo: construir un pipeline ETL que procese CSV crudos de clientes, productos y ventas, aplique limpieza, JOIN, validaciones y exporte un dataset listo para anÃ¡lisis.

---

## ðŸ“‚ Dataset (crudo)

UbicaciÃ³n: data/data_raw/

* clientes.csv
* productos.csv
* ventas.csv

---

## ðŸ”„ Pipeline (diagrama simplificado)

clientes.csv  â†’ Limpieza clientes  \
productos.csv â†’ Limpieza productos  ---> JOIN ventas + productos + clientes â†’ Validaciones de datos â†’ Export CSV/Parquet â†’ Automatizacion (Semana 5)
ventas.csv    â†’ Limpieza ventas   /

---

## ðŸ”¹ Actividad 3 â€“ Supuestos del sistema

Como Data Engineer Ãºnico en la empresa, defino los siguientes supuestos:

* Los archivos CSV llegan una vez por dÃ­a por correo (6 AM).
* Los archivos pueden venir vacÃ­os, duplicados o con errores de formato.
* Los IDs de cliente y producto son Ãºnicos y sirven como claves de uniÃ³n.
* Si un cliente tiene email invÃ¡lido, igual se carga pero se marca como pendiente.
* El negocio necesita reportes diarios de ingresos y ventas por producto.
* Los archivos histÃ³ricos no cambian: una vez cargados, se consideran fijos y no se reescriben.

---

## ðŸ“¸ Flujo detallado (ASCII)

[CSV crudos] â†’ [Lectura con pandas] â†’ [Limpieza + Validaciones] â†’ [JOIN clientes + productos + ventas] â†’ [Parquet/CSV limpio] â†’ [Dashboard BI]

---

## âœ… Checklist de avance

* Lectura de CSV (pandas)
* Limpieza por dataset (tipos, fechas, precios, nulos)
* JOIN final (ventas + productos + clientes)
* Validaciones (nulos, duplicados, FKs, rangos)
* Export limpio (data/clean/ en CSV o Parquet)
* AutomatizaciÃ³n (cron/Step Functions) â€“ Semana 5

---

## ðŸš€ CÃ³mo correr:

1. Crear entorno virtual: python -m venv venv
2. Activar: venv\Scripts\activate
3. Instalar dependencias: pip install -r requirements.txt
4. Ejecutar: python pipeline/etl_local.py

---

